{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 label classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJOZjV_Gr7ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vkuyrFQbEDc",
        "colab_type": "text"
      },
      "source": [
        "\"Irrelevant\" class - raw data & pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_u1owA5sZzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading the csv files containing tweets for \"Irrelevant\" class. \n",
        "\n",
        "ibb_anlamsiz = pd.read_csv('/data/ibb_anlamsiz.csv', encoding = 'cp1254')\n",
        "ibb_siyasi = pd.read_csv('/data/ibb_siyasi.csv', encoding = 'cp1254')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8pKTHIRvmC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0ec61456-c96b-4d0b-f949-b08b1f891803"
      },
      "source": [
        "!pip install TurkishStemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting TurkishStemmer\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/bf/3e56dd4ce442f9237e1c202ce736ae5e5818d74f81604f1665e67736cfc0/TurkishStemmer-1.3-py3-none-any.whl\n",
            "Installing collected packages: TurkishStemmer\n",
            "Successfully installed TurkishStemmer-1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTu5tGEYuxeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c1f6b236-0ba8-4065-9ddd-e07988618d37"
      },
      "source": [
        "import nltk\n",
        "from TurkishStemmer import TurkishStemmer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LafmuKaVu_3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This is the preprocessing for tweets. We have additional processes such as removal of incomplete words at the end of tweets, usernames \n",
        "and RT (denoting retweets) to remove extra evidence about irrelevant tweets that are not related to the content.\"\"\"\n",
        "\n",
        "porter = TurkishStemmer()\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('turkish')\n",
        "\n",
        "def clean_text(string):\n",
        "\n",
        "    message = re.sub(r'\\w+\\…',\" \", string) #Removal of incomplete words\n",
        "    message = re.sub(r'@\\w+(:)?',\" \", message) #Removal of Twitter user names\n",
        "    message = re.sub(r'\\ART', \" \", message) #Removal of RT\n",
        "    message = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', message)\n",
        "    message = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', ' ', #Replace URLs with space because it might be too freq in this class\n",
        "                     message)\n",
        "    message = re.sub(r'₺|\\$', 'money', message) #Replace money symbols with 'money'\n",
        "    message = re.sub(\n",
        "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', #Replace phone numbers with 'phonenumbr'\n",
        "        'phonenumbr', message)\n",
        "    message = re.sub(r'\\d+(\\.\\d+)?', 'numbr', message)  #Replace numbers with 'numbr'\n",
        "    message = re.sub(r'[^\\w\\d\\s]', ' ', message) #Punctuation removed\n",
        "    message = re.sub(r'\\s+', ' ', message) #Too much space replaced by single space\n",
        "    message = re.sub(r'^\\s+|\\s+?$', '', message.lower()) #Get rid of spaces at the beginning and at the end.\n",
        "    return ' '.join(\n",
        "    porter.stem(term)\n",
        "    for term in message.split()\n",
        "    if term not in set(stop_words)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCaaIkgdwCEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "558cb8d8-90dd-4d51-efb2-289b117aa947"
      },
      "source": [
        "\"\"\"Creating the dataset for tweets labelled as 'Irrelevant'. We apply pre-processing, arrange the columns\n",
        "and add the label \"4\" to denote 'Irrelevant'.\"\"\"\n",
        "\n",
        "ibb_ilgisiz = pd.concat([ibb_anlamsiz, ibb_siyasi], axis = 0)\n",
        "\n",
        "textCopy = ibb_ilgisiz['Text']\n",
        "textCopy = textCopy.apply(clean_text)\n",
        "ibb_ilgisiz[\"Text\"] = textCopy\n",
        "ibb_ilgisiz.reset_index(inplace = True)\n",
        "ibb_ilgisiz.drop(['index', 'Author', 'Date'], axis = 1, inplace = True)\n",
        "ibb_ilgisiz['Label'] = 4\n",
        "ibb_ilgisiz.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2713, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFqYwOcQwgpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train-validation and test split for 'Irrelevant' dataset. \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tr_irrel_text, tst_irrel_text, tr_irrel_lbl, tst_irrel_lbl = train_test_split(ibb_ilgisiz.drop('Label', axis = 1), ibb_ilgisiz['Label'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "tr_irrel_text, val_irrel_text, tr_irrel_lbl, val_irrel_lbl = train_test_split(tr_irrel_text, tr_irrel_lbl, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZsqXI8QcfD5",
        "colab_type": "text"
      },
      "source": [
        "Municipality's enterprises data - raw data & pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjxotmsqxMKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading the files that contain complaints to municipality's enterprises. We already had them as split into train-valid-test sets. \n",
        "\n",
        "tr_organisations = pd.read_csv('/data/ibb_organisations_train.csv')\n",
        "val_organisations = pd.read_csv('/data/ibb_organisations_valid.csv')\n",
        "tst_organisations = pd.read_csv('/data/ibb_organisations_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_t8-cdtxmFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing procedure for complaints to municipality's enterprises\n",
        "\n",
        "porter = TurkishStemmer()\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('turkish')\n",
        "\n",
        "def clean_text2(string):\n",
        "    message = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(string))\n",
        "    message = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', ' ', #Replace URLs with ' '\n",
        "                     message)\n",
        "    message = re.sub(r'₺|\\$', 'money', message) #Replace money symbols with 'money'\n",
        "    message = re.sub(\n",
        "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', #Replace phone numbers with 'phonenumbr'\n",
        "        'phonenumbr', message)\n",
        "    message = re.sub(r'\\d+(\\.\\d+)?', 'numbr', message)  #Replace numbers with 'numbr'\n",
        "    message = re.sub(r'[^\\w\\d\\s]', ' ', message)\n",
        "    message = re.sub(r'\\s+', ' ', message)\n",
        "    message = re.sub(r'^\\s+|\\s+?$', '', message.lower())\n",
        "    return ' '.join(\n",
        "    porter.stem(term)\n",
        "    for term in message.split()\n",
        "    if term not in set(stop_words)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVFIFgeLx0eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pre-processing implemented on train-valid-test data sets of municipality's enterprises. \n",
        "\n",
        "textCopy = tr_organisations['comment']\n",
        "textCopy = textCopy.apply(clean_text2)\n",
        "tr_organisations[\"comment\"] = textCopy\n",
        "\n",
        "textCopy1 = val_organisations['comment']\n",
        "textCopy1 = textCopy1.apply(clean_text2)\n",
        "val_organisations[\"comment\"] = textCopy1\n",
        "\n",
        "textCopy1 = tst_organisations['comment']\n",
        "textCopy1 = textCopy1.apply(clean_text2)\n",
        "tst_organisations[\"comment\"] = textCopy1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j_ZN2mq09n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Arranging column names.\n",
        "\n",
        "tr_organisations.columns = ['Text', 'Label']\n",
        "val_organisations.columns = ['Text', 'Label']\n",
        "tst_organisations.columns = ['Text', 'Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyzB_EXwc9wT",
        "colab_type": "text"
      },
      "source": [
        "Combining \"Irrelevant\" class with the enterprises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T057uTBdzVoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combining labels of irrelevant class with that of enterprises \n",
        "\n",
        "tr_irrel_lbl = pd.DataFrame(tr_irrel_lbl, columns = ['Label'])\n",
        "val_irrel_lbl = pd.DataFrame(val_irrel_lbl, columns = ['Label'])\n",
        "tst_irrel_lbl = pd.DataFrame(tst_irrel_lbl, columns = ['Label'])\n",
        "\n",
        "y_train = pd.concat([tr_organisations['Label'], tr_irrel_lbl['Label']], axis = 0, ignore_index = True)\n",
        "y_valid = pd.concat([val_organisations['Label'], val_irrel_lbl['Label']], axis = 0, ignore_index = True)\n",
        "y_test = pd.concat([tst_organisations['Label'], tst_irrel_lbl['Label']], axis = 0, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJBMmeeYzuSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combining texts of irrelevant class with that of enterprises\n",
        "\n",
        "new_train = pd.concat([tr_organisations['Text'], tr_irrel_text['Text']], axis = 0, ignore_index = True)\n",
        "new_valid = pd.concat([val_organisations['Text'], val_irrel_text['Text']], axis = 0, ignore_index = True)\n",
        "new_test = pd.concat([tst_organisations['Text'], tst_irrel_text['Text']], axis = 0, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWajypiR02Fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding up text and label columns + shuffling the datasets\n",
        "\n",
        "import random\n",
        "\n",
        "new_train_with_y = pd.concat([new_train, y_train], axis = 1)\n",
        "new_valid_with_y = pd.concat([new_valid, y_valid], axis = 1)\n",
        "new_test_with_y = pd.concat([new_test, y_test], axis = 1)\n",
        "\n",
        "new_train_with_y = new_train_with_y.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
        "new_valid_with_y = new_valid_with_y.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
        "new_test_with_y = new_test_with_y.sample(frac = 1, random_state = 42).reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srVBsNpG1kUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separating test and labels for train-valid-test each and updating new_train, new_valid, new_test\n",
        "\n",
        "new_train = new_train_with_y['Text']\n",
        "y_train = new_train_with_y['Label']\n",
        "\n",
        "new_valid = new_valid_with_y['Text']\n",
        "y_valid = new_valid_with_y['Label']\n",
        "\n",
        "new_test = new_test_with_y['Text']\n",
        "y_test = new_test_with_y['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xoHNrcfeKbo",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW27H_-Q1nzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing the sentences\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "vocab_fit = count_vectorizer.fit_transform(new_train.values)\n",
        "train_vec = pd.DataFrame(vocab_fit.toarray(), columns=count_vectorizer.get_feature_names(), index=new_train.index)\n",
        "\n",
        "vocab_fit = count_vectorizer.transform(new_valid.values)\n",
        "validation1_vec = pd.DataFrame(vocab_fit.toarray(), columns=count_vectorizer.get_feature_names(), index=new_valid.index)\n",
        "\n",
        "vocab_fit = count_vectorizer.transform(new_test.values)\n",
        "test_vec = pd.DataFrame(vocab_fit.toarray(), columns=count_vectorizer.get_feature_names(), index=new_test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INjBdq6x1qnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a165185c-c8ff-4121-dee8-5972f5b4b2d5"
      },
      "source": [
        "#Removing exceptionally short sentences from the datasets\n",
        "\n",
        "train_vec['sum'] = train_vec.apply(sum, axis = 1)\n",
        "validation1_vec['sum'] = validation1_vec.apply(sum, axis = 1)\n",
        "test_vec['sum'] = test_vec.apply(sum, axis = 1)\n",
        "\n",
        "train_vec2 = train_vec.loc[train_vec['sum'] >= 5]\n",
        "validation1_vec2 = validation1_vec.loc[validation1_vec['sum'] >= 5]\n",
        "test_vec2 = test_vec.loc[test_vec['sum'] >= 5]\n",
        "\n",
        "y_train2 = y_train.loc[train_vec['sum'] >= 5]\n",
        "y_valid2 = y_valid.loc[validation1_vec['sum'] >= 5]\n",
        "y_test2 = y_test.loc[test_vec['sum'] >= 5]\n",
        "\n",
        "train_vec2.drop('sum', axis = 1, inplace = True)\n",
        "validation1_vec2.drop('sum', axis = 1, inplace = True)\n",
        "test_vec2.drop('sum', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPc1WbJ92Z3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will need these word_vectors later for find_label(). \n",
        "\n",
        "total_vec = pd.concat([train_vec2, validation1_vec2, test_vec2], axis = 0)\n",
        "y_labels = pd.concat([y_train2, y_valid2, y_test2], axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqmWugeekfZ",
        "colab_type": "text"
      },
      "source": [
        "ML algorithms and their results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0G-mUIv25-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "9f798fe1-84e4-4bb3-9249-d57ad54e2580"
      },
      "source": [
        "#LogisticRegression results\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "lr_model = LogisticRegression(dual = True, solver = 'liblinear', max_iter = 5000)\n",
        "lr_model.fit(train_vec2, y_train2)\n",
        "y_pred_lr = lr_model.predict(validation1_vec2)\n",
        "\n",
        "print(classification_report(y_valid2, y_pred_lr))\n",
        "print('\\n')\n",
        "print(confusion_matrix(y_valid2, y_pred_lr))\n",
        "print('\\n')\n",
        "print('Accuracy:', accuracy_score(y_valid2, y_pred_lr))\n",
        "print('\\n')\n",
        "print('f1 score:', f1_score(y_valid2, y_pred_lr, average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       532\n",
            "           1       0.95      0.96      0.95       480\n",
            "           2       0.93      0.90      0.91       441\n",
            "           3       0.95      0.94      0.94       433\n",
            "           4       0.97      0.99      0.98       419\n",
            "\n",
            "    accuracy                           0.95      2305\n",
            "   macro avg       0.95      0.95      0.95      2305\n",
            "weighted avg       0.95      0.95      0.95      2305\n",
            "\n",
            "\n",
            "\n",
            "[[509   0  16   3   4]\n",
            " [  0 459   4  13   4]\n",
            " [ 27  11 395   6   2]\n",
            " [  4  13   8 405   3]\n",
            " [  2   2   0   0 415]]\n",
            "\n",
            "\n",
            "Accuracy: 0.9470715835140998\n",
            "\n",
            "\n",
            "f1 score: 0.9468666100334894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmS2dONJ2-zb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "53884c26-8721-4bcf-d635-b6945702c467"
      },
      "source": [
        "#SGDClassifier Results \n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_model = SGDClassifier(random_state = 42)\n",
        "sgd_model.fit(train_vec2, y_train2)\n",
        "y_pred_sgd = sgd_model.predict(validation1_vec2)\n",
        "\n",
        "print(classification_report(y_valid2, y_pred_sgd))\n",
        "print('\\n')\n",
        "print(confusion_matrix(y_valid2, y_pred_sgd))\n",
        "print('\\n')\n",
        "print('Accuracy:', accuracy_score(y_valid2, y_pred_sgd))\n",
        "print('\\n')\n",
        "print('f1 score:', f1_score(y_valid2, y_pred_sgd, average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95       532\n",
            "           1       0.93      0.96      0.94       480\n",
            "           2       0.95      0.88      0.91       441\n",
            "           3       0.95      0.93      0.94       433\n",
            "           4       0.98      0.99      0.98       419\n",
            "\n",
            "    accuracy                           0.94      2305\n",
            "   macro avg       0.95      0.94      0.94      2305\n",
            "weighted avg       0.94      0.94      0.94      2305\n",
            "\n",
            "\n",
            "\n",
            "[[514   1  11   4   2]\n",
            " [  2 459   3  14   2]\n",
            " [ 31  14 388   5   3]\n",
            " [  6  14   8 402   3]\n",
            " [  2   4   0   0 413]]\n",
            "\n",
            "\n",
            "Accuracy: 0.9440347071583514\n",
            "\n",
            "\n",
            "f1 score: 0.9437647483346557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKvH959-3pmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "edfc9ab2-8325-4ee9-ac72-ab1dd182aa77"
      },
      "source": [
        "#Pipeline created for Random Forest\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "\n",
        "pipe = Pipeline([('classifier', RandomForestClassifier(random_state = 42))])\n",
        "\n",
        "# Create param grid.\n",
        "\n",
        "param_grid = [\n",
        "    {'classifier' : [RandomForestClassifier(random_state = 42)],\n",
        "    'classifier__n_estimators' : list(range(10,101,10)),\n",
        "    'classifier__max_features' : list(range(6,32,5))}\n",
        "]\n",
        "\n",
        "# Create grid search object\n",
        "\n",
        "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "\n",
        "# Fit on data\n",
        "\n",
        "best_clf = clf.fit(train_vec2, y_train2)\n",
        "y_pred_clf = clf.predict(validation1_vec2)\n",
        "\n",
        "\n",
        "print(classification_report(y_valid2, y_pred_clf))\n",
        "print('\\n')\n",
        "print(confusion_matrix(y_valid2, y_pred_clf))\n",
        "print('\\n')\n",
        "print('Accuracy:', accuracy_score(y_valid2, y_pred_clf))\n",
        "print('\\n')\n",
        "print('f1 score:', f1_score(y_valid2, y_pred_clf, average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 18.8min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 30.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.96      0.90       532\n",
            "           1       0.91      0.96      0.93       480\n",
            "           2       0.94      0.74      0.83       441\n",
            "           3       0.97      0.86      0.91       433\n",
            "           4       0.93      0.99      0.96       419\n",
            "\n",
            "    accuracy                           0.91      2305\n",
            "   macro avg       0.92      0.90      0.91      2305\n",
            "weighted avg       0.91      0.91      0.91      2305\n",
            "\n",
            "\n",
            "\n",
            "[[513   0  13   1   5]\n",
            " [  4 462   0   8   6]\n",
            " [ 83  19 327   4   8]\n",
            " [ 13  27   8 373  12]\n",
            " [  1   2   0   0 416]]\n",
            "\n",
            "\n",
            "Accuracy: 0.9071583514099784\n",
            "\n",
            "\n",
            "f1 score: 0.9053327314264936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_igyUWk64FSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6a30b1b5-3a7c-412a-ae74-afd7b8445ef7"
      },
      "source": [
        "#Multinomial NB results\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mb_model = MultinomialNB()\n",
        "mb_model.fit(train_vec2, y_train2)\n",
        "\n",
        "y_pred_mb = mb_model.predict(validation1_vec2) \n",
        "\n",
        "print(classification_report(y_valid2, y_pred_mb))\n",
        "print('\\n')\n",
        "print(confusion_matrix(y_valid2, y_pred_mb))\n",
        "print('\\n')\n",
        "print('Accuracy:', accuracy_score(y_valid2, y_pred_mb))\n",
        "print('\\n')\n",
        "print('f1 score:', f1_score(y_valid2, y_pred_mb, average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90       532\n",
            "           1       0.94      0.97      0.95       480\n",
            "           2       0.90      0.78      0.83       441\n",
            "           3       0.89      0.96      0.93       433\n",
            "           4       1.00      0.88      0.94       419\n",
            "\n",
            "    accuracy                           0.91      2305\n",
            "   macro avg       0.92      0.91      0.91      2305\n",
            "weighted avg       0.91      0.91      0.91      2305\n",
            "\n",
            "\n",
            "\n",
            "[[505   1  20   6   0]\n",
            " [  1 466   0  13   0]\n",
            " [ 72  14 344  11   0]\n",
            " [  4   9   5 415   0]\n",
            " [  7   8  15  19 370]]\n",
            "\n",
            "\n",
            "Accuracy: 0.911062906724512\n",
            "\n",
            "\n",
            "f1 score: 0.9102597923197292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaCTC-IC_RmK",
        "colab_type": "text"
      },
      "source": [
        "Demonstration of find_label function in examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry1b5urs_xRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Regular preprocessing for normal texts (for demonstration purposes) - now as a generator\n",
        "\n",
        "porter = TurkishStemmer()\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('turkish')\n",
        "\n",
        "def clean_text3(string):\n",
        "    message = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(string))\n",
        "    message = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', ' ', #Replace URLs with space\n",
        "                     message)\n",
        "    message = re.sub(r'₺|\\$', 'money', message) #Replace money symbols with 'money'\n",
        "    message = re.sub(\n",
        "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', #Replace phone numbers with 'phonenumbr'\n",
        "        'phonenumbr', message)\n",
        "    message = re.sub(r'\\d+(\\.\\d+)?', 'numbr', message)  #Replace numbers with 'numbr'\n",
        "    message = re.sub(r'[^\\w\\d\\s]', ' ', message)\n",
        "    message = re.sub(r'\\s+', ' ', message)\n",
        "    message = re.sub(r'^\\s+|\\s+?$', '', message.lower())\n",
        "    yield ' '.join(\n",
        "    porter.stem(term)\n",
        "    for term in message.split()\n",
        "    if term not in set(stop_words)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PfbMKyy_FSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function applies pre-processing on any string it receives, vectorizes it and tries to predict its class with MultinomialNB. \n",
        "\n",
        "def find_label(sentence):\n",
        "\n",
        "  sentence = [sentence]\n",
        "\n",
        "  new_sentence = clean_text3(sentence)\n",
        "\n",
        "  new_vocab_vec = count_vectorizer.transform(new_sentence)\n",
        "  \n",
        "  new_pred = mb_model.predict(new_vocab_vec)\n",
        "\n",
        "  if new_pred == 0:\n",
        "    return ('İgdaş (Gas distribution & billing)')\n",
        "  elif new_pred == 1:\n",
        "    return ('İett (Public transportation)')\n",
        "  elif new_pred == 2:\n",
        "    return ('İski (Water distribution & billing)')\n",
        "  elif new_pred == 3:\n",
        "    return ('Diğer İBB iştirakleri (Other enterprises)')\n",
        "  elif new_pred == 4:\n",
        "    return ('İlgisiz (Irrelevant)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvwb11Hd_fRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c839d26a-5c29-416e-cfb8-fee7a13c9e54"
      },
      "source": [
        "#The sentence below would be translated as \"Rudeness of the bus driver\" - which would concern İett. \n",
        "\n",
        "find_label('Otobüs şoförünün büyük ayıbı')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'İett (Public transportation)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9A2DJRn_llR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79fe50f8-c8d9-4bcf-9bbf-3fca0078651b"
      },
      "source": [
        "#The sentence below would be translated as \"Exorbitant bills for gas\" - which would concern İgdaş.\n",
        "\n",
        "find_label('Fahiş doğalgaz faturası!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'İgdaş (Gas distribution & billing)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}